{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashpandey030303/SLM-Quantization-research-implementation/blob/main/Final_SLM_Quantisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Step 1: Forcefully uninstalling old libraries ---\")\n",
        "!pip uninstall -y transformers accelerate"
      ],
      "metadata": {
        "id": "UQ2PTtockRYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Step 1.1: Installing the correct, latest libraries ---\")\n",
        "!pip install -q \"accelerate>=0.28.0\"\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q datasets bitsandbytes"
      ],
      "metadata": {
        "id": "3hJ-tpsVkRSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "id": "3W7N-7q-kfIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Libraries installed and imported successfully!\\n\")\n"
      ],
      "metadata": {
        "id": "2xhIHUIUauNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Step 2: Configuring models ---\")\n",
        "model_ids = [\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "]\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "print(\"Model IDs and quantization config are set.\\n\")"
      ],
      "metadata": {
        "id": "pBBcc-p9az-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Step 3: Loading and preparing the XNLI dataset ---\")\n",
        "\n",
        "# Load the first 50 samples from the English and Hindi validation sets directly.\n",
        "try:\n",
        "    en_samples = load_dataset(\"xnli\", \"en\", split=\"validation\").select(range(50))\n",
        "    hi_samples = load_dataset(\"xnli\", \"hi\", split=\"validation\").select(range(50))\n",
        "    xnli_samples = {\"en\": en_samples, \"hi\": hi_samples}\n",
        "\n",
        "    # The labels are numeric (0=entailment, 1=neutral, 2=contradiction). We map them to text for the model.\n",
        "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "\n",
        "    print(f\"Loaded {len(en_samples)} English and {len(hi_samples)} Hindi samples.\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the dataset: {e}\")\n",
        "    print(\"Please ensure you have a stable internet connection.\")"
      ],
      "metadata": {
        "id": "JVgFoO1dcC0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model_id, quantization_config=None):\n",
        "    \"\"\"\n",
        "    Loads a model and evaluates its performance on the XNLI samples.\n",
        "    Measures accuracy, average latency, and peak GPU memory usage.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading tokenizer for {model_id}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # Set device_map: Quantized models MUST be on GPU. Baseline can be auto (may offload to CPU).\n",
        "    device_map_setting = \"cuda\" if quantization_config else \"auto\"\n",
        "    print(f\"Loading model {model_id} with device_map='{device_map_setting}' and {'4-bit quantization' if quantization_config else 'no quantization'}...\")\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=device_map_setting,\n",
        "            trust_remote_code=True,\n",
        "            attn_implementation=\"eager\", # Use SDPA for potentially better memory/speed and stability\n",
        "            torch_dtype=torch.bfloat16 # Use bfloat16 for Phi-3 baseline if not quantized, for better memory\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to load model {model_id}. This is often due to OOM or incompatibilities.\")\n",
        "        print(f\"Details: {e}\")\n",
        "        return [] # Return empty list to prevent further errors for this model\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\n--- Evaluating Model: {model_id} ({'4-bit Quantized' if quantization_config else 'Baseline'}) ---\")\n",
        "\n",
        "    for lang, samples in xnli_samples.items():\n",
        "        correct_predictions = 0\n",
        "        total_latency = 0\n",
        "\n",
        "        # Reset GPU memory stats for accurate measurement per language\n",
        "        # This will only be relevant if the model itself fits on GPU\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        for i, example in enumerate(samples):\n",
        "            premise = example['premise']\n",
        "            hypothesis = example['hypothesis']\n",
        "            true_label = label_map[example['label']]\n",
        "\n",
        "            prompt = (\n",
        "                f\"Premise: '{premise}'\\n\"\n",
        "                f\"Hypothesis: '{hypothesis}'\\n\"\n",
        "                \"Based on the premise, does the hypothesis mean entailment, neutral, or contradiction? \"\n",
        "                \"Answer with only one word.\"\n",
        "            )\n",
        "\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "            # Add attention_mask to avoid warning and for more reliable results\n",
        "            # And use_cache=False to avoid DynamicCache related errors, as discussed\n",
        "            start_time = time.time()\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=5,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                attention_mask=inputs.attention_mask if hasattr(inputs, 'attention_mask') else None,\n",
        "                use_cache=False # Crucial for stability with potentially problematic caches, and for small outputs like 5 tokens, performance hit is minimal.\n",
        "            )\n",
        "            end_time = time.time()\n",
        "            total_latency += (end_time - start_time)\n",
        "\n",
        "            response_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).lower().strip()\n",
        "\n",
        "            if true_label in response_text:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            # Print progress\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == len(samples): # Print every 10 samples or at the end\n",
        "                print(f\"  [{lang.upper()}] Sample {i+1}/{len(samples)} | Pred: '{response_text}' | True: '{true_label}'\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = (correct_predictions / len(samples)) * 100\n",
        "        avg_latency = total_latency / len(samples)\n",
        "        peak_memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0 # Handle CPU offload\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_id.split('/')[-1],\n",
        "            \"Type\": \"4-bit Quantized\" if quantization_config else \"Baseline\",\n",
        "            \"Language\": lang.upper(),\n",
        "            \"Accuracy (%)\": f\"{accuracy:.2f}\",\n",
        "            \"Avg Latency (s/ex)\": f\"{avg_latency:.4f}\",\n",
        "            \"Peak VRAM (MB)\": f\"{peak_memory_mb:.2f}\"\n",
        "        })\n",
        "\n",
        "    # Clear memory after evaluation for this model\n",
        "    del model\n",
        "    del tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"Evaluation function is ready.\\n\")"
      ],
      "metadata": {
        "id": "yGtQkl-RcTZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "mQsLt9Dk09Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Step 5: Starting experiments incrementally ---\")\n",
        "results_filename = \"slm_quantization_results.csv\"\n",
        "\n",
        "# Clear previous results file if it exists\n",
        "if os.path.exists(results_filename):\n",
        "    os.remove(results_filename)\n",
        "    print(f\"Removed existing '{results_filename}'.\")\n",
        "\n",
        "# Header for the CSV file\n",
        "csv_header = [\"Model\", \"Type\", \"Language\", \"Accuracy (%)\", \"Avg Latency (s/ex)\", \"Peak VRAM (MB)\"]"
      ],
      "metadata": {
        "id": "9BYi-oXgcTWh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Running Experiment 1: Phi-3 Baseline ---\")\n",
        "model_id_phi = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "phi_baseline_results = evaluate_model(model_id_phi, quantization_config=None)\n",
        "df_phi_baseline = pd.DataFrame(phi_baseline_results, columns=csv_header)\n",
        "df_phi_baseline.to_csv(results_filename, mode='w', header=True, index=False)\n",
        "print(f\"✅ Phi-3 Baseline results saved to {results_filename}\")"
      ],
      "metadata": {
        "id": "heaXQmgzj5mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Running Experiment 1: Phi-3 Quantized ---\")\n",
        "model_id_phi = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "phi_baseline_results = evaluate_model(model_id_phi, quantization_config=quantization_config)\n",
        "df_phi_baseline = pd.DataFrame(phi_baseline_results, columns=csv_header)\n",
        "df_phi_baseline.to_csv(results_filename, mode='w', header=True, index=False)\n",
        "print(f\"✅ Phi-3 Baseline results saved to {results_filename}\")"
      ],
      "metadata": {
        "id": "MI8TS5pJx4f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment 3: Qwen Baseline ---\n",
        "print(\"\\n--- Running Experiment 3: Qwen Baseline ---\")\n",
        "model_id_qwen = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "qwen_baseline_results = evaluate_model(model_id_qwen, quantization_config=None)\n",
        "df_qwen_baseline = pd.DataFrame(qwen_baseline_results, columns=csv_header)\n",
        "df_qwen_baseline.to_csv(results_filename, mode='a', header=False, index=False)\n",
        "print(f\"✅ Qwen Baseline results appended to {results_filename}\")"
      ],
      "metadata": {
        "id": "MuS59rFTx4ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment 4: Qwen Quantized ---\n",
        "print(\"\\n--- Running Experiment 4: Qwen Quantized ---\")\n",
        "model_id_qwen = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "qwen_quantized_results = evaluate_model(model_id_qwen, quantization_config=quantization_config)\n",
        "df_qwen_quantized = pd.DataFrame(qwen_quantized_results, columns=csv_header)\n",
        "df_qwen_quantized.to_csv(results_filename, mode='a', header=False, index=False)\n",
        "print(f\"✅ Qwen Quantized results appended to {results_filename}\")"
      ],
      "metadata": {
        "id": "T9svwpwwx4aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Final Results Table ---\")\n",
        "\n",
        "try:\n",
        "    final_results_df = pd.read_csv(results_filename)\n",
        "    print(final_results_df.to_string())\n",
        "except FileNotFoundError:\n",
        "    print(\"The results file was not found. This should not happen if all experiments ran correctly.\")"
      ],
      "metadata": {
        "id": "PBVGO-ZZx4Xz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}